{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Normal EPANET .inp File to a SWMM .inp file assuming Flow-Restricted Withdrawal (FRW)\n",
    "This notebook takes an input EPANET file with demands input normally as a CWS base demand and outputs a .inp file configured for SWMM and uses a FRW assumption  \n",
    "This conversion is a modified version of the method presented by Campisano et al. (2018) [1] and posited by Cabrera-Bejar & Tzatchkov (2009) [2]  \n",
    "A simplified schematic of the modified demand node in this method (SWMM-FR) is seen below:  \n",
    "\n",
    "  ![](SWMM-FR.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we import the necessary libraries and packages\n",
    "**WNTR** for building EPANET network models in Python  \n",
    "**NUMPY & PANDAS** for data handling and processing  \n",
    "**re** for searching and matching text in the .inp file using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wntr\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying paths for EPANET.inp File to be Converted and preprocessing the input\n",
    "**Warning:** *Paths in this script (and the rest of this repo) are absolute unless running the network files provided within the repo*  \n",
    "Input filename (with extensions) as string.  \n",
    "For running the .inp files in this repository, you can use this relative path `\"../Network-Files/Network X/\"` where X is the network number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:  Network3_12hr_\n"
     ]
    }
   ],
   "source": [
    " # Replace with appropriate path and filename\n",
    "directory='/Users/omaraliamer/Desktop/UofT/Publications/How to Model IWS/Github/IWS-Modelling-Methods-Repo/Network-Files/Network 3/'\n",
    "filename='Network3_12hr_PDA.inp'\n",
    "name_only=filename[0:-7]\n",
    "print(\"Selected File: \",name_only)\n",
    "abs_path=directory+filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Assumptions Input\n",
    "Converting a CWS demand-driven analysis into an IWS pressure-driven analysis requires some assumptions in all methods  \n",
    "The resistance of the service connection between the demand junction and the household (end-user) is uncertain and is modelled using two assumptions  \n",
    "The **desired head (pressure)** is the pressure at the demand junction at which (or above) the consumer can satisfy their full demand in the supply duration (or possible less)  \n",
    "The **minimum head (pressure)** is the minimum pressure at the demand junction required for flow to begin passing through the service connection  \n",
    "These two assumptions dictate the flow-pressure relationship that determines the pressure-dependent flow through the service connection as follows:\n",
    "\n",
    "$$ Q\\, = \\!Q_{des}\\sqrt{\\frac{H_{j}-H^{min}}{H^{des}-H^{min}}} \\quad[1]$$ \n",
    "Where Q is the flow through the service connection, $Q_{des}$ is the desired (base) demand, $H_j$ is the head at the demand junction $j$, $H^{min}$ is the minimum head, and $H^{des}$ is the desired head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_pressure=10     # Set the desired pressure\n",
    "minimum_pressure=0      # Set the minimum pressure\n",
    "pressure_diff=desired_pressure-minimum_pressure  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information from the EPANET file\n",
    "To modify the .inp file, demand junction IDs, elevations, x and y coordinates  \n",
    "We use wntr to build the network model of the input file and use wntr's junctions module to extract the details of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_nodes=[]       # For storing list of nodes that have non-zero demands\n",
    "desired_demands=[]    # For storing demand rates desired by each node for desired volume calculations\n",
    "elevations=[]         # For storing elevations of demand nodes\n",
    "coords=dict()         # For storing coordinates corresponding to each node as a tuple with the id as key\n",
    "all_nodes=[]          # For storing list of node ids of all nodes\n",
    "all_elevations=[]     # For storing elevations of all nodes\n",
    "## MAYBE SAVE ALL NODE IDS IN DATAFRAME WITH ELEVATION AND BASE DEMAND AND THEN FILTER DATA FRAME LATER FOR DEMAND NODES ONLY\n",
    "\n",
    "# Creates a network model object using EPANET .inp file\n",
    "network=wntr.network.WaterNetworkModel(abs_path)\n",
    "\n",
    "# Iterates over the junction list in the Network object\n",
    "for node in network.junctions():\n",
    "    all_nodes.append(node[1].name)\n",
    "    all_elevations.append(node[1].elevation)\n",
    "    coords[node[1].name]=node[1].coordinates\n",
    "    # For all nodes that have non-zero demands\n",
    "    if node[1].base_demand != 0:\n",
    "        # Record node ID (name), desired demand (base_demand) in CMS, elevations, x and y coordinates\n",
    "        demand_nodes.append(node[1].name)\n",
    "        desired_demands.append(node[1].base_demand)\n",
    "        elevations.append(node[1].elevation)\n",
    "        \n",
    "\n",
    "conduit_ids= []\n",
    "conduit_from= []\n",
    "conduit_to= []\n",
    "conduit_lengths= []\n",
    "conduit_diameters= []\n",
    "for link in network.links():\n",
    "    conduit_ids.append(link[1].name)\n",
    "    conduit_from.append(link[1].start_node_name)\n",
    "    conduit_to.append(link[1].end_node_name)\n",
    "    conduit_lengths.append(link[1].length)\n",
    "    conduit_diameters.append(link[1].diameter)\n",
    "\n",
    "reservoir_ids=[]\n",
    "reservoir_heads={}\n",
    "reservoir_coords={}\n",
    "for reservoir in network.reservoirs():\n",
    "    reservoir_ids.append(reservoir[1].name)\n",
    "    reservoir_heads[reservoir_ids[-1]]=reservoir[1].base_head\n",
    "    reservoir_coords[reservoir_ids[-1]]=reservoir[1].coordinates\n",
    "\n",
    "\n",
    "# Get the supply duration in minutes (/60) as an integer\n",
    "supply_duration=int(network.options.time.duration/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Junctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDepth=[0]*len(all_nodes)\n",
    "InitDepth=MaxDepth\n",
    "SurDepth=[100] * len(all_nodes)  # High value to prevent surcharging\n",
    "Aponded=MaxDepth\n",
    "\n",
    "orig_junctions=pd.DataFrame(list(zip(all_nodes,all_elevations,MaxDepth,InitDepth,SurDepth,Aponded)))\n",
    "orig_junctions=orig_junctions.to_string(header=False,index=False,col_space=10).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Storage Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_elevations=[0]*len(reservoir_ids)\n",
    "MaxDepth=[max(100,max(reservoir_heads.values())+10)]*len(reservoir_ids)\n",
    "InitDepth=reservoir_heads.values()\n",
    "reservoir_shape=[\"FUNCTIONAL\"]*len(reservoir_ids)\n",
    "reservoir_coeff=[0]*len(reservoir_ids)\n",
    "reservoir_expon=[0]*len(reservoir_ids)\n",
    "reservoir_const=[1000000]*len(reservoir_ids)\n",
    "reservoir_fevap=reservoir_expon\n",
    "reservoir_psi=reservoir_fevap\n",
    "\n",
    "storage_section=pd.DataFrame(zip(reservoir_ids,reservoir_elevations,MaxDepth,InitDepth,reservoir_shape,reservoir_coeff,reservoir_expon,reservoir_const,reservoir_fevap,reservoir_psi))\n",
    "storage_section=storage_section.to_string(header=False,index=False,col_space=10).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfall_ids=[\"Outfall\"+str(id) for id in desired_demands]\n",
    "outfall_elevations=elevations\n",
    "outfall_type=[\"FREE\"]*len(outfall_ids)\n",
    "stage_data=[\"   \"]*len(outfall_ids)\n",
    "outfall_gated=[\"NO\"]*len(outfall_ids)\n",
    "\n",
    "outfall_section=pd.DataFrame(zip(outfall_ids,outfall_elevations,outfall_type,stage_data,outfall_gated))\n",
    "outfall_section=outfall_section.to_string(header=False,index=False,col_space=10).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outlets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_ids = [\"Outlet\"+id for id in demand_nodes]\n",
    "outlet_from = demand_nodes\n",
    "outlet_to = outfall_ids\n",
    "outlet_offset=[0]*len(outlet_ids)\n",
    "outlet_type=[\"TABULAR/DEPTH\"]*len(outlet_ids)\n",
    "outlet_qtable=[str(round(demand*1000000)) for demand in desired_demands]  # To generate unique Table IDs for each demand rate (not demand node) i.e., juncitons with the same demand are assigned the same outlet curve\n",
    "outlet_expon=[\"    \"]*len(outlet_ids)\n",
    "outlet_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "outlets=pd.DataFrame(list(zip(outlet_ids,outlet_from,outlet_to,outlet_offset,outlet_type,outlet_qtable,outlet_expon,outlet_gated)))\n",
    "outlets=outlets.to_string(header=False,index=False,col_space=10).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outlet Curves\n",
    "Example:  \n",
    "60               Rating     0          0           \n",
    "60                          2          0.026832816  \n",
    "60                          4          0.037947332  \n",
    "60                          6          0.0464758   \n",
    "60                          8          0.053665631  \n",
    "60                          10         0.06        \n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ids=list(set(outlet_qtable))   # removes duplicates from list\n",
    "curves_name=[]\n",
    "curves_type=[]\n",
    "curves_x=[]\n",
    "curves_y=[]\n",
    "for table in table_ids:\n",
    "    demand=int(table)/1000                # in LPS\n",
    "    for depth in np.arange(0,11,1):\n",
    "        curves_name.append(table)\n",
    "        if depth==0:\n",
    "            curves_type.append(\"Rating\")\n",
    "        else: curves_type.append(\" \")\n",
    "        curves_x.append(depth)\n",
    "        curves_y.append(demand*np.sqrt((depth-minimum_pressure)/(desired_pressure-minimum_pressure)))\n",
    "    curves_name.append(\";\")\n",
    "    curves_type.append(\" \")\n",
    "    curves_x.append(\" \")\n",
    "    curves_y.append(\" \")\n",
    "\n",
    "curves=pd.DataFrame(list(zip(curves_name,curves_type,curves_x,curves_y)))\n",
    "curves=curves.to_string(header=False,index=False,col_space=10).splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERPOLATING \n",
    "$$ N_{parts}= \\left\\lceil \\frac{L_{pipe}}{\\Delta x_{max}} \\right\\rceil\\\\\n",
    "L_{part}=\\frac{L_{pipe}}{N_{parts}}\\\\\n",
    "\\Delta E = \\frac{ E_{end}-E_{start}}{N_{parts}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of conduit allowed\n",
    "maximum_xdelta=10\n",
    "junctions=pd.DataFrame(zip(all_nodes,all_elevations,coords.values()),columns=[\"ID\",\"Elevation\",\"Coordinates\"])\n",
    "junctions.set_index(\"ID\",inplace=True)\n",
    "\n",
    "conduits=pd.DataFrame(zip(conduit_ids,conduit_from,conduit_to,conduit_lengths,conduit_diameters),columns=[\"ID\",\"from node\",\"to node\",\"Length\",\"diameter\"])\n",
    "conduits.set_index(\"ID\",inplace=True)\n",
    "\n",
    "# Loop over each conduit in the original file\n",
    "for conduit in conduits.index:\n",
    "\n",
    "    length=conduits[\"Length\"][conduit]  #Stores the length of the current conduit for shorthand\n",
    "\n",
    "    # If the conduit is bigger than the maximum allowable length (delta x), we will break it down into smaller pipes\n",
    "    if length>maximum_xdelta:\n",
    "        # Number of smaller pipes is calculated from \n",
    "        n_parts=math.ceil(length/maximum_xdelta)\n",
    "        # Calculate the length of each part \n",
    "        part_length=length/n_parts\n",
    "        # Start node ID (for shorthand)\n",
    "        start_node=conduits[\"from node\"][conduit]\n",
    "        # End node ID (for shorthand)\n",
    "        end_node=conduits[\"to node\"][conduit]\n",
    "        # If the start node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # MAke the start elevation the same as the end but add 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            start_elevation=end_elevation+1\n",
    "        # Otherwise make the start elevation equal to the elevation of the start node\n",
    "        else: start_elevation=junctions.at[start_node,\"Elevation\"]\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # MAke the end elevation the same as the start but subtract 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            end_elevation=start_elevation-1\n",
    "        # Make the end elevation equal to the elevation of the end node\n",
    "        else: end_elevation=junctions.at[end_node,\"Elevation\"]\n",
    "        # Calculate the uniform drop (or rise) in elevation for all the intermediate nodes about to be created when this pipe is broken into several smaller ones\n",
    "        unit_elev_diff=(end_elevation-start_elevation)/n_parts\n",
    "\n",
    "\n",
    "        if start_node in reservoir_ids:\n",
    "            start_x=reservoir_coords[start_node][0]\n",
    "            start_y=reservoir_coords[start_node][1]\n",
    "        else:\n",
    "            start_x=junctions.at[start_node,\"Coordinates\"][0]\n",
    "            start_y=junctions.at[start_node,\"Coordinates\"][1]\n",
    "        \n",
    "\n",
    "        if end_node in reservoir_ids:\n",
    "            end_x=reservoir_coords[end_node][0]\n",
    "            end_y=reservoir_coords[end_node][1]\n",
    "        else:\n",
    "            end_x=junctions.at[end_node,\"Coordinates\"][0]\n",
    "            end_y=junctions.at[end_node,\"Coordinates\"][1]\n",
    "            \n",
    "\n",
    "        unit_x_diff=(end_x-start_x)/n_parts\n",
    "        unit_y_diff=(end_y-start_y)/n_parts\n",
    "\n",
    "\n",
    "# THIS LOOP GENERATES THE SMALLER PIPES TO REPLACE THE ORIGINAL LONG PIPE\n",
    "        # For each part to be created\n",
    "        for part in np.arange(1,n_parts+1):\n",
    "\n",
    "            # CREATING THE LINKS\n",
    "            # Create the ID for the new smaller pipe as OriginPipeID-PartNumber\n",
    "            new_id=conduit+\"-\"+str(part)\n",
    "            # Set the new pipe's diameter equal to the original one\n",
    "            conduits.at[new_id,\"diameter\"]=conduits[\"diameter\"][conduit]\n",
    "            # Set the start node as OriginStartNode-NewNodeNumber-OriginEndNode  as in the first intermediate nodes between node 13 and 14 will be named 13-1-14\n",
    "            conduits.at[new_id,\"from node\"]=start_node+\"-\"+str(part-1)+\"-\"+end_node\n",
    "            # if this is the first part, use the original start node \n",
    "            if part==1:\n",
    "                conduits.at[new_id,\"from node\"]=start_node\n",
    "            # Set the end node as OriginStartNode-NewNodeNumber+1-OriginEndNode  as in the second intermediate nodes between node 13 and 14 will be named 13-2-14\n",
    "            conduits.at[new_id,\"to node\"]=start_node+\"-\"+str(part)+\"-\"+end_node\n",
    "            # If this is the last part, use the original end node as the end node\n",
    "            if part==n_parts:\n",
    "                conduits.at[new_id,\"to node\"]=end_node\n",
    "            # Set the new pipe's length to the length of each part\n",
    "            conduits.at[new_id,\"Length\"]=part_length\n",
    "\n",
    "            if part<n_parts:\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Elevation\"]=start_elevation+part*unit_elev_diff\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Coordinates\"]=(start_x+part*unit_x_diff,start_y+part*unit_y_diff)\n",
    "\n",
    "    # After writing the new smaller pipes, delete the original pipe (redundant)\n",
    "    conduits.drop(conduit,inplace=True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Conduit Section\n",
    "NAME    FROM NODE   TO NODE   Length   Roughness    InOffset     OutOffset     InitFlow    MaxFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "manning=[0.011]*len(conduits.index)\n",
    "InOffset=[0]*len(conduits.index)\n",
    "OutOffset=InOffset\n",
    "InitFlow=InOffset\n",
    "MaxFlow=InOffset\n",
    "\n",
    "conduit_section=pd.DataFrame(zip(conduits.index,conduits[\"from node\"],conduits[\"to node\"],conduits[\"Length\"],manning,InOffset,OutOffset,InitFlow,MaxFlow))\n",
    "conduit_section=conduit_section.to_string(index=False,header=False,col_space=10).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing X-Sections\n",
    "Link    Shape     Geom1 (DIAMETER)    Geom2 (HW Coefficient)     Geom3    Geom 4       Barrels     Culvert (EMPTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=[\"FORCE_MAIN\"]*len(conduits.index)\n",
    "hwcoeffs=[130]*len(shape)\n",
    "geom3=[0]*len(shape)\n",
    "geom4=geom3\n",
    "nbarrels=[1]*len(shape)\n",
    "\n",
    "xsections_section=pd.DataFrame(zip(conduits.index,shape,conduits[\"diameter\"],hwcoeffs,geom3,geom4,nbarrels))\n",
    "xsections_section=xsections_section.to_string(header=False,index=False, col_space=10).splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd6a0ae2e89424b22bc0c3fd46707be6f981ef35bdac29f85707eba2fc78ecd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

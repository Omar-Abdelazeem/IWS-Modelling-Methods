{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Normal EPANET .inp File to a SWMM .inp file assuming Flow-Restricted Withdrawal (FRW)\n",
    "This notebook takes an input EPANET file with demands input normally as a CWS base demand and outputs a .inp file configured for SWMM and uses a FRW assumption  \n",
    "This conversion is a modified version of the method presented by Campisano et al. (2018) [1] and posited by Cabrera-Bejar & Tzatchkov (2009) [2]  \n",
    "A simplified schematic of the modified demand node in this method (SWMM-FR) is seen below:  \n",
    "\n",
    "  ![](SWMM-FR.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we import the necessary libraries and packages\n",
    "**WNTR** for building EPANET network models in Python  \n",
    "**NUMPY & PANDAS** for data handling and processing  \n",
    "**re** for searching and matching text in the .inp file using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wntr\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying paths for EPANET.inp File to be Converted and preprocessing the input\n",
    "**Warning:** *Paths in this script (and the rest of this repo) are absolute unless running the network files provided within the repo*  \n",
    "Input filename (with extensions) as string.  \n",
    "For running the .inp files in this repository, you can use this relative path `\"../Network-Files/Network X/\"` where X is the network number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:  Network3_12hr_\n"
     ]
    }
   ],
   "source": [
    " # Replace with appropriate path and filename\n",
    "directory='/Users/omaraliamer/Desktop/UofT/Publications/How to Model IWS/Github/IWS-Modelling-Methods-Repo/Network-Files/Network 3/'\n",
    "filename='Network3_12hr_PDA.inp'\n",
    "name_only=filename[0:-7]\n",
    "print(\"Selected File: \",name_only)\n",
    "abs_path=directory+filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Assumptions Input\n",
    "Converting a CWS demand-driven analysis into an IWS pressure-driven analysis requires some assumptions in all methods  \n",
    "The resistance of the service connection between the demand junction and the household (end-user) is uncertain and is modelled using two assumptions  \n",
    "The **desired head (pressure)** is the pressure at the demand junction at which (or above) the consumer can satisfy their full demand in the supply duration (or possible less)  \n",
    "The **minimum head (pressure)** is the minimum pressure at the demand junction required for flow to begin passing through the service connection  \n",
    "These two assumptions dictate the flow-pressure relationship that determines the pressure-dependent flow through the service connection as follows:\n",
    "\n",
    "$$ Q\\, = \\!Q_{des}\\sqrt{\\frac{H_{j}-H^{min}}{H^{des}-H^{min}}} \\quad[1]$$ \n",
    "Where Q is the flow through the service connection, $Q_{des}$ is the desired (base) demand, $H_j$ is the head at the demand junction $j$, $H^{min}$ is the minimum head, and $H^{des}$ is the desired head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_pressure=10     # Set the desired pressure\n",
    "minimum_pressure=0      # Set the minimum pressure\n",
    "pressure_diff=desired_pressure-minimum_pressure  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information from the EPANET file\n",
    "To modify the .inp file, demand junction IDs, elevations, x and y coordinates  \n",
    "We use wntr to build the network model of the input file and use wntr's junctions module to extract the details of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_nodes=[]       # For storing list of nodes that have non-zero demands\n",
    "desired_demands=[]    # For storing demand rates desired by each node for desired volume calculations\n",
    "elevations=[]         # For storing elevations of demand nodes\n",
    "coords=dict()         # For storing coordinates corresponding to each node as a tuple with the id as key\n",
    "all_nodes=[]          # For storing list of node ids of all nodes\n",
    "all_elevations=[]     # For storing elevations of all nodes\n",
    "## MAYBE SAVE ALL NODE IDS IN DATAFRAME WITH ELEVATION AND BASE DEMAND AND THEN FILTER DATA FRAME LATER FOR DEMAND NODES ONLY\n",
    "\n",
    "# Creates a network model object using EPANET .inp file\n",
    "network=wntr.network.WaterNetworkModel(abs_path)\n",
    "\n",
    "# Iterates over the junction list in the Network object\n",
    "for node in network.junctions():\n",
    "    all_nodes.append(node[1].name)\n",
    "    all_elevations.append(node[1].elevation)\n",
    "    coords[node[1].name]=node[1].coordinates\n",
    "    # For all nodes that have non-zero demands\n",
    "    if node[1].base_demand != 0:\n",
    "        # Record node ID (name), desired demand (base_demand) in CMS, elevations, x and y coordinates\n",
    "        demand_nodes.append(node[1].name)\n",
    "        desired_demands.append(node[1].base_demand)\n",
    "        elevations.append(node[1].elevation)\n",
    "        \n",
    "\n",
    "conduit_ids= []       # To store IDs of the original pipes in the EPANET file\n",
    "conduit_from= []      # To store the origin node for each pipe\n",
    "conduit_to= []        # To store the destination node for each pipe\n",
    "conduit_lengths= []   # To store pipe lengths\n",
    "conduit_diameters= [] # To store pipe diameters\n",
    "\n",
    "# Loop over each link in the EPANET model\n",
    "for link in network.links():\n",
    "\n",
    "    # Extract and store each of the aforementioned properties\n",
    "    conduit_ids.append(link[1].name)\n",
    "    conduit_from.append(link[1].start_node_name)\n",
    "    conduit_to.append(link[1].end_node_name)\n",
    "    conduit_lengths.append(link[1].length)\n",
    "    conduit_diameters.append(link[1].diameter)\n",
    "\n",
    "reservoir_ids=[]      # To store the source reservoirs' IDs\n",
    "reservoir_heads={}    # To store the total head of each reservoir indexed by ID\n",
    "reservoir_coords={}   # To store the coordinates as tuple (x,y) indexed by ID\n",
    "\n",
    "# Loops over each reservoir\n",
    "for reservoir in network.reservoirs():\n",
    "    reservoir_ids.append(reservoir[1].name)\n",
    "    reservoir_heads[reservoir_ids[-1]]=reservoir[1].base_head\n",
    "    reservoir_coords[reservoir_ids[-1]]=reservoir[1].coordinates\n",
    "\n",
    "\n",
    "# Get the supply duration in minutes (/60) as an integer\n",
    "supply_duration=int(network.options.time.duration/60)\n",
    "supply_hh=str(supply_duration//60)     # The hour value of the supply duration (quotient of total supply in minutes/ 60)\n",
    "supply_mm=str(supply_duration%60)      # The minute value of the supply duration (remainder)\n",
    "\n",
    "# Corrects the formatting of the HH:MM by adding a 0 if it is a single digit: if minutes =4 -> 04\n",
    "if len(supply_mm)<2:\n",
    "    supply_mm='0'+supply_mm\n",
    "if len(supply_hh)<2:\n",
    "    supply_hh='0'+supply_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Discretization of Longer Pipes\n",
    "To improve the stability and accuracy of SWMM in complex networks, longer pipes should be discretized into smaller ones  \n",
    "Refer to [3] for a more in-depth discussion of this procedure  \n",
    "The following cell breaks down pipes that are longer than a specified maximum $\\Delta x_{max}$  \n",
    "Pipes longer than the maximum length are divided into equal parts, where the number of parts is:   \n",
    "$$ N_{parts}= \\left\\lceil \\frac{L_{pipe}}{\\Delta x_{max}} \\right\\rceil$$\n",
    "The length of each part is then set at:\n",
    "$$L_{part}=\\frac{L_{pipe}}{N_{parts}}$$\n",
    "After creating all pipe segments, intermediate nodes are created to join the pipe segments.  \n",
    "The elevation of these nodes (as well as the x and y coordinates) are interpolated linearly using the start elevation and the end elevation,  \n",
    " where the difference in elevation between each two consecutive nodes set as:\n",
    "$$\\Delta E = \\frac{ E_{end}-E_{start}}{N_{parts}}$$\n",
    "Similarly, the difference in x and y coordinates are found as:  \n",
    "$$\\Delta x = \\frac{ x_{end}-x_{start}}{N_{parts}}\\\\\n",
    "\\Delta y = \\frac{ y_{end}-y_{start}}{N_{parts}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of conduit allowed\n",
    "maximum_xdelta=10\n",
    "\n",
    "# Dataframe aggregating all node information gathered from the EPANET file\n",
    "junctions=pd.DataFrame(zip(all_nodes,all_elevations,coords.values()),columns=[\"ID\",\"Elevation\",\"Coordinates\"])\n",
    "# Set the junction ID as the index of the Dataframe\n",
    "junctions.set_index(\"ID\",inplace=True)\n",
    "\n",
    "# Dataframe aggregating all conduit information gathered from the EPANET file\n",
    "conduits=pd.DataFrame(zip(conduit_ids,conduit_from,conduit_to,conduit_lengths,conduit_diameters),columns=[\"ID\",\"from node\",\"to node\",\"Length\",\"diameter\"])\n",
    "# Set the conduit ID as the index\n",
    "conduits.set_index(\"ID\",inplace=True)\n",
    "\n",
    "# Loop over each conduit in the original file\n",
    "for conduit in conduits.index:\n",
    "\n",
    "    length=conduits[\"Length\"][conduit]  #Stores the length of the current conduit for shorthand\n",
    "\n",
    "    # If the conduit is bigger than the maximum allowable length (delta x), we will break it down into smaller pipes\n",
    "    if length>maximum_xdelta:\n",
    "        # Number of smaller pipes is calculated from \n",
    "        n_parts=math.ceil(length/maximum_xdelta)\n",
    "        # Calculate the length of each part \n",
    "        part_length=length/n_parts\n",
    "        # Start node ID (for shorthand)\n",
    "        start_node=conduits[\"from node\"][conduit]\n",
    "        # End node ID (for shorthand)\n",
    "        end_node=conduits[\"to node\"][conduit]\n",
    "        # If the start node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # MAke the start elevation the same as the end but add 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            start_elevation=junctions.at[end_node,\"Elevation\"]+1\n",
    "        # Otherwise make the start elevation equal to the elevation of the start node\n",
    "        else: start_elevation=junctions.at[start_node,\"Elevation\"]\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # MAke the end elevation the same as the start but subtract 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            end_elevation=start_elevation-1\n",
    "        # Make the end elevation equal to the elevation of the end node\n",
    "        else: end_elevation=junctions.at[end_node,\"Elevation\"]\n",
    "        # Calculate the uniform drop (or rise) in elevation for all the intermediate nodes about to be created when this pipe is broken into several smaller ones\n",
    "        unit_elev_diff=(end_elevation-start_elevation)/n_parts\n",
    "\n",
    "        # if the starting node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # Get coordinates from reservoir data\n",
    "            start_x=reservoir_coords[start_node][0]\n",
    "            start_y=reservoir_coords[start_node][1]\n",
    "        else:\n",
    "            # Get the coordinates from the junction data\n",
    "            start_x=junctions.at[start_node,\"Coordinates\"][0]\n",
    "            start_y=junctions.at[start_node,\"Coordinates\"][1]\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # Get the coordinates from the reservoir data\n",
    "            end_x=reservoir_coords[end_node][0]\n",
    "            end_y=reservoir_coords[end_node][1]\n",
    "        else:\n",
    "            # Get them from the junctions data\n",
    "            end_x=junctions.at[end_node,\"Coordinates\"][0]\n",
    "            end_y=junctions.at[end_node,\"Coordinates\"][1]\n",
    "            \n",
    "        # Calculate the unit difference in x and y coordinates for this pipe and its segments\n",
    "        unit_x_diff=(end_x-start_x)/n_parts\n",
    "        unit_y_diff=(end_y-start_y)/n_parts\n",
    "\n",
    "\n",
    "# THIS LOOP GENERATES THE SMALLER PIPES TO REPLACE THE ORIGINAL LONG PIPE\n",
    "        # For each part to be created\n",
    "        for part in np.arange(1,n_parts+1):\n",
    "\n",
    "            # CREATING THE LINKS\n",
    "            # Create the ID for the new smaller pipe as OriginPipeID-PartNumber\n",
    "            new_id=conduit+\"-\"+str(part)\n",
    "            # Set the new pipe's diameter equal to the original one\n",
    "            conduits.at[new_id,\"diameter\"]=conduits[\"diameter\"][conduit]\n",
    "            # Set the start node as OriginStartNode-NewNodeNumber-OriginEndNode  as in the first intermediate nodes between node 13 and 14 will be named 13-1-14\n",
    "            conduits.at[new_id,\"from node\"]=start_node+\"-\"+str(part-1)+\"-\"+end_node\n",
    "            # if this is the first part, use the original start node \n",
    "            if part==1:\n",
    "                conduits.at[new_id,\"from node\"]=start_node\n",
    "            # Set the end node as OriginStartNode-NewNodeNumber+1-OriginEndNode  as in the second intermediate nodes between node 13 and 14 will be named 13-2-14\n",
    "            conduits.at[new_id,\"to node\"]=start_node+\"-\"+str(part)+\"-\"+end_node\n",
    "            # If this is the last part, use the original end node as the end node\n",
    "            if part==n_parts:\n",
    "                conduits.at[new_id,\"to node\"]=end_node\n",
    "            # Set the new pipe's length to the length of each part\n",
    "            conduits.at[new_id,\"Length\"]=part_length\n",
    "\n",
    "            # if this is NOT the last part (as the last pipe segment joins a pre-existing node and does not need a node to be created)\n",
    "            if part<n_parts:\n",
    "                # Create a new node at the end of this pipe segment whose elevation is translated from the start elevation using the unit slope and the part number\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Elevation\"]=start_elevation+part*unit_elev_diff\n",
    "                # Calculate the coordinates for the new node using the unit difference in x and y coordinates\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Coordinates\"]=(start_x+part*unit_x_diff,start_y+part*unit_y_diff)\n",
    "\n",
    "        # After writing the new smaller pipes, delete the original pipe (since it is now redundant)\n",
    "        conduits.drop(conduit,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Junctions\n",
    "The junction section lines are written. Data required for the junctions include:\n",
    "Name: Already stored in junctions  \n",
    "Elevation: Already stored in junctions  \n",
    "MaxDepth: 0  \n",
    "Initial Depth (InitDepth): 0 No initialization required  \n",
    "Surcharge Depth (SurDepth): 100 or any value high enough to prevent the node from overflowing (to simulate a pressurized pipe)  \n",
    "Area Ponded (Aponded): 0  No ponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDepth=[0]*len(junctions)\n",
    "InitDepth=MaxDepth\n",
    "SurDepth=[100] * len(junctions)  # High value to prevent surcharging\n",
    "Aponded=InitDepth\n",
    "\n",
    "# Creates dataframe with each row representing one line from the junctions section\n",
    "junctions_section=pd.DataFrame(list(zip(junctions.index,junctions[\"Elevation\"],MaxDepth,InitDepth,SurDepth,Aponded)))\n",
    "# Converts the dataframe into a list of lines in the junctions section\n",
    "junctions_section=junctions_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "# adds a new line character to the end of each line in the section\n",
    "junctions_section=[line+'\\n' for line in junctions_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outfalls\n",
    "Data required for outfalls:  \n",
    "Name: formatted as OutfallX where X is the ID of the original demand node  \n",
    "Elevation: equal to the original demand node's elevation  \n",
    "Type: FREE no boundary conditions forced  \n",
    "Stage Data: Blank None provided  \n",
    "Gated: NO  \n",
    "Route To: Blank None specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Outfall to each demand node ID\n",
    "outfall_ids=[\"Outfall\"+str(id) for id in demand_nodes]\n",
    "# Same as the demand nodes\n",
    "outfall_elevations=elevations\n",
    "# Free outfalls\n",
    "outfall_type=[\"FREE\"]*len(outfall_ids)\n",
    "# Blank Stage Data\n",
    "stage_data=[\"   \"]*len(outfall_ids)\n",
    "# Not gated\n",
    "outfall_gated=[\"NO\"]*len(outfall_ids)\n",
    "\n",
    "# Creates dataframe with each row representing one line from the outfalls section\n",
    "outfall_section=pd.DataFrame(zip(outfall_ids,outfall_elevations,outfall_type,stage_data,outfall_gated))\n",
    "# Converts the dataframe into a list of lines in the outfalls section\n",
    "outfall_section=outfall_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "# adds a new line character to the end of each line in the section\n",
    "outfall_section=[line+'\\n' for line in outfall_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Storage Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_elevations=[0]*len(reservoir_ids)\n",
    "MaxDepth=[max(100,max(reservoir_heads.values())+10)]*len(reservoir_ids)\n",
    "InitDepth=reservoir_heads.values()\n",
    "reservoir_shape=[\"FUNCTIONAL\"]*len(reservoir_ids)\n",
    "reservoir_coeff=[0]*len(reservoir_ids)\n",
    "reservoir_expon=[0]*len(reservoir_ids)\n",
    "reservoir_const=[1000000]*len(reservoir_ids)\n",
    "reservoir_fevap=reservoir_expon\n",
    "reservoir_psi=reservoir_fevap\n",
    "\n",
    "storage_section=pd.DataFrame(zip(reservoir_ids,reservoir_elevations,MaxDepth,InitDepth,reservoir_shape,reservoir_coeff,reservoir_expon,reservoir_const,reservoir_fevap,reservoir_psi))\n",
    "storage_section=storage_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "storage_section=[line+'\\n' for line in storage_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Conduits\n",
    "Name From To L Roughness(manning) InOff  OutOff InitFlow  MaxFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughness=[0.011]*len(conduits)\n",
    "conduit_zeros=[0]*len(conduits)\n",
    "\n",
    "conduits_section=pd.DataFrame(zip(conduits.index,conduits[\"from node\"],conduits[\"to node\"],conduits[\"Length\"],roughness,conduit_zeros,conduit_zeros,conduit_zeros,conduit_zeros))\n",
    "conduits_section=conduits_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "conduits_section=[line+'\\n' for line in conduits_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outlets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_ids = [\"Outlet\"+id for id in demand_nodes]\n",
    "outlet_from = demand_nodes\n",
    "outlet_to = outfall_ids\n",
    "outlet_offset=[0]*len(outlet_ids)\n",
    "outlet_type=[\"TABULAR/DEPTH\"]*len(outlet_ids)\n",
    "outlet_qtable=[str(round(demand*1000000)) for demand in desired_demands]  # To generate unique Table IDs for each demand rate (not demand node) i.e., juncitons with the same demand are assigned the same outlet curve\n",
    "outlet_expon=[\"    \"]*len(outlet_ids)\n",
    "outlet_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "outlets=pd.DataFrame(list(zip(outlet_ids,outlet_from,outlet_to,outlet_offset,outlet_type,outlet_qtable,outlet_expon,outlet_gated)))\n",
    "outlet_section=outlets.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "outlet_section=[line+'\\n' for line in outlet_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing X-Sections\n",
    "Link    Shape     Geom1 (DIAMETER)    Geom2 (HW Coefficient)     Geom3    Geom 4       Barrels     Culvert (EMPTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=[\"FORCE_MAIN\"]*len(conduits.index)\n",
    "hwcoeffs=[130]*len(shape)\n",
    "geom3=[0]*len(shape)\n",
    "geom4=geom3\n",
    "nbarrels=[1]*len(shape)\n",
    "\n",
    "xsections_section=pd.DataFrame(zip(conduits.index,shape,conduits[\"diameter\"],hwcoeffs,geom3,geom4,nbarrels))\n",
    "xsections_section=xsections_section.to_string(header=False,index=False, col_space=10).splitlines()\n",
    "xsections_section=[line+'\\n' for line in xsections_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outlet Curves\n",
    "Example:  \n",
    "60               Rating     0          0           \n",
    "60                          2          0.026832816  \n",
    "60                          4          0.037947332  \n",
    "60                          6          0.0464758   \n",
    "60                          8          0.053665631  \n",
    "60                          10         0.06        \n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ids=list(set(outlet_qtable))   # removes duplicates from list\n",
    "curves_name=[]\n",
    "curves_type=[]\n",
    "curves_x=[]\n",
    "curves_y=[]\n",
    "for table in table_ids:\n",
    "    demand=int(table)/1000                # in LPS\n",
    "    for depth in np.arange(0,11,1):\n",
    "        curves_name.append(table)\n",
    "        if depth==0:\n",
    "            curves_type.append(\"Rating\")\n",
    "        else: curves_type.append(\" \")\n",
    "        curves_x.append(depth)\n",
    "        curves_y.append(demand*np.sqrt((depth-minimum_pressure)/(desired_pressure-minimum_pressure)))\n",
    "    curves_name.append(\";\")\n",
    "    curves_type.append(\" \")\n",
    "    curves_x.append(\" \")\n",
    "    curves_y.append(\" \")\n",
    "\n",
    "curves=pd.DataFrame(list(zip(curves_name,curves_type,curves_x,curves_y)))\n",
    "curves_section=curves.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "curves_section=[line+'\\n' for line in curves_section]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_demand= { node: coords[node] for node in demand_nodes}\n",
    "coords_ids=list(junctions.index)+reservoir_ids+outfall_ids\n",
    "\n",
    "coords_x1=[coord[0] for coord in junctions[\"Coordinates\"]]\n",
    "coords_x2=[coord[0] for coord in reservoir_coords.values()]\n",
    "coords_x3=[coord[0] +20 for coord in coords_demand.values()]\n",
    "coords_x=coords_x1+coords_x2+coords_x3\n",
    "\n",
    "coords_y1=[coord[1] for coord in junctions[\"Coordinates\"]]\n",
    "coords_y2=[coord[1] for coord in reservoir_coords.values()]\n",
    "coords_y3=[coord[1] +20 for coord in coords_demand.values()]\n",
    "coords_y=coords_y1+coords_y2+coords_y3\n",
    "\n",
    "coordinate_section=pd.DataFrame(zip(coords_ids,coords_x,coords_y))\n",
    "coordinate_section=coordinate_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "coordinate_section=[line+'\\n' for line in coordinate_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the SWMM .inp File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens .inp file to read\n",
    "file=open('/Users/omaraliamer/Desktop/UofT/Publications/How to Model IWS/Github/IWS-Modelling-Methods-Repo/Network-Files/Empty_SWMM_Template.inp','r')\n",
    "lines=[]              # list to store all lines in the .inp file\n",
    "linecount=0           # Counter for the number of lines\n",
    "end_time=0\n",
    "junctions_marker=0    # To store the line number at which the junctions section starts\n",
    "outfalls_marker=0     # To store the line number at which the emitter section starts\n",
    "storage_marker=0      # To store the line number at which the pumps section starts\n",
    "conduits_marker=0     # to store the line number at which the valves section\n",
    "outlets_marker=0      # To store the line number at which the vertices section starts\n",
    "xsections_marker=0    # To store the line number of teh emitter exponent option\n",
    "curves_marker=0\n",
    "coords_marker=0\n",
    "\n",
    "# Loops over each line in the input file \n",
    "for line in file:\n",
    "    if re.search(\"^END_TIME\",line):\n",
    "        end_time=linecount\n",
    "    # Record the position of the phrase [JUNCTIONS] and add 2 to skip the header line\n",
    "    if re.search('\\[JUNCTIONS\\]',line):\n",
    "        junctions_marker=linecount+3\n",
    "    # Record the position of the phrase [TANKS] and add 2 to skip the header line\n",
    "    if re.search('\\[OUTFALLS\\]',line):\n",
    "        outfalls_marker=linecount+3\n",
    "     # Record the position of the phrase [PUMPS] and subtract 1 to add pipes to the end of the pipe section\n",
    "    if re.search('\\[STORAGE\\]',line):\n",
    "        storage_marker=linecount+3\n",
    "    # Record the position of the phrase [VALVES] and add 2 to skip the header line\n",
    "    if re.search('\\[CONDUITS\\]',line):\n",
    "        conduits_marker=linecount+3\n",
    "     # Record the position of the phrase [Vertices] and subtract 1 to add Tank cooridnates to the end of the coordinates section\n",
    "    if re.search('\\[OUTLETS\\]',line):\n",
    "        outlets_marker=linecount+3\n",
    "     # Record the position of the phrase [Vertices] and subtract 1 to add Tank cooridnates to the end of the coordinates section\n",
    "    if re.search('\\[XSECTIONS\\]',line):\n",
    "        xsections_marker=linecount+3\n",
    "    # Record the position of the phrase [Vertices] and subtract 1 to add Tank cooridnates to the end of the coordinates section\n",
    "    if re.search('\\[CURVES\\]',line):\n",
    "        curves_marker=linecount+3\n",
    "    # Record the position of the phrase [Vertices] and subtract 1 to add Tank cooridnates to the end of the coordinates section\n",
    "    if re.search('\\[COORDINATES\\]',line):\n",
    "        coords_marker=linecount+3\n",
    "    # Store all lines in a list\n",
    "    lines.append(line)\n",
    "    linecount+=1\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfalls_marker+=len(junctions)\n",
    "storage_marker+=len(outfall_section)+len(junctions)\n",
    "conduits_marker+=len(storage_section)+len(outfall_section)+len(junctions)\n",
    "outlets_marker+=len(conduits_section)+len(storage_section)+len(outfall_section)+len(junctions)\n",
    "xsections_marker+=len(outlet_section)+len(conduits_section)+len(storage_section)+len(outfall_section)+len(junctions)\n",
    "curves_marker+=len(xsections_section)+len(outlet_section)+len(conduits_section)+len(storage_section)+len(outfall_section)+len(junctions)\n",
    "coords_marker+=len(curves_section)+len(xsections_section)+len(outlet_section)+len(conduits_section)+len(storage_section)+len(outfall_section)+len(junctions)\n",
    "\n",
    "\n",
    "file=open(directory+name_only+'SWMM-FRW.inp','w')\n",
    "lines[end_time]=\"END_TIME             \"+str(supply_hh)+\":\"+str(supply_mm)+\":00\\n\"\n",
    "lines[junctions_marker:junctions_marker]=junctions_section\n",
    "lines[outfalls_marker:outfalls_marker]=outfall_section\n",
    "lines[storage_marker:storage_marker]=storage_section\n",
    "lines[conduits_marker:conduits_marker]=conduits_section\n",
    "lines[outlets_marker:outlets_marker]=outlet_section\n",
    "lines[xsections_marker:xsections_marker]=xsections_section\n",
    "lines[curves_marker:curves_marker]=curves_section\n",
    "lines[coords_marker:coords_marker]=coordinate_section\n",
    "\n",
    "\n",
    "# All lines added by this script are missing a new line character at the end, the conditional statements below add the new line character for these lines only and writes all lines to the file\n",
    "for line in lines:\n",
    "    file.write(line)    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<div class=\"csl-entry\"> [1] Campisano, A., Gullotta, A., &#38; Modica, C. (2018). Using EPA-SWMM to simulate intermittent water distribution systems. <i>Urban Water Journal</i>, <i>15</i>(10), 925–933. https://doi.org/10.1080/1573062X.2019.1597379</div>\n",
    "<div class=\"csl-entry\"> [2] Cabrera-Bejar, J. A., &#38; Tzatchkov, V. G. (2009). Inexpensive Modeling of Intermittent Service Water Distribution Networks. <i>World Environmental and Water Resources Congress 2009</i>, 1–10. https://doi.org/10.1061/41036(342)29</div>\n",
    "<div class=\"csl-entry\">[3] Pachaly, R. L., Vasconcelos, J. G., Allasia, D. G., &#38; Bocchi, J. P. P. (2022). Evaluating SWMM capabilities to simulate closed pipe transients. <i>Journal of Hydraulic Research</i>, <i>60</i>(1), 74–81. https://doi.org/10.1080/00221686.2020.1866695</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

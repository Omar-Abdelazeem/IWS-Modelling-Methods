{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Normal EPANET .inp File to a SWMM .inp file assuming Volume-Restricted Withdrawal (Outlet-Storage)\n",
    "This notebook takes an input EPANET file with demands input normally as a CWS base demand and outputs a .inp file configured for SWMM and uses a volume-restricted assumption  \n",
    "A simplified schematic of the modified demand node in this method (Outlet-Storage) is seen below:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we import the necessary libraries and packages\n",
    "**WNTR** for building EPANET network models in Python  \n",
    "**NUMPY & PANDAS** for data handling and processing  \n",
    "**re** for searching and matching text in the .inp file using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wntr\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying paths for EPANET.inp File to be Converted and preprocessing the input\n",
    "**Warning:** *Paths in this script (and the rest of this repo) are absolute unless running the network files provided within the repo*  \n",
    "Input filename (with extensions) as string.  \n",
    "For running the .inp files in this repository, you can use this relative path `\"../Network-Files/Network X/\"` where X is the network number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:  \\Network2_12hr\n"
     ]
    }
   ],
   "source": [
    " # Replace with appropriate path and filename\n",
    "directory=r\"C:\\IWS_Modelling\\Github\\IWS-Modelling-Methods-Repo\\Network-Files\\Network 2\"\n",
    "filename=r'\\Network2_12hr_PDA.inp'\n",
    "name_only=filename.rsplit('_',maxsplit=1)[0]\n",
    "print(\"Selected File: \",name_only)\n",
    "abs_path=directory+filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Assumptions Input\n",
    "Converting a CWS demand-driven analysis into an IWS pressure-driven analysis requires some assumptions in all methods  \n",
    "The resistance of the service connection between the demand junction and the household (end-user) is uncertain and is modelled using two assumptions  \n",
    "The **desired head (pressure)** is the pressure at the demand junction at which (or above) the consumer can satisfy their full demand in the supply duration (or possible less)  \n",
    "The **minimum head (pressure)** is the minimum pressure at the demand junction required for flow to begin passing through the service connection  \n",
    "These two assumptions dictate the flow-pressure relationship that determines the pressure-dependent flow through the service connection as follows:\n",
    "\n",
    "$$ Q\\, = \\!Q_{des}\\sqrt{\\frac{H_{j}-H^{min}}{H^{des}-H^{min}}} \\quad[1]$$ \n",
    "Where Q is the flow through the service connection, $Q_{des}$ is the desired (base) demand, $H_j$ is the head at the demand junction $j$, $H^{min}$ is the minimum head, and $H^{des}$ is the desired head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_pressure=10     # Set the desired pressure\n",
    "minimum_pressure=0      # Set the minimum pressure\n",
    "pressure_diff=desired_pressure-minimum_pressure  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information from the EPANET file\n",
    "To modify the .inp file, demand junction IDs, elevations, x and y coordinates  \n",
    "We use wntr to build the network model of the input file and use wntr's junctions module to extract the details of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_nodes=[]       # For storing list of nodes that have non-zero demands\n",
    "desired_demands=[]    # For storing demand rates desired by each node for desired volume calculations\n",
    "elevations=[]         # For storing elevations of demand nodes\n",
    "coords=dict()         # For storing coordinates corresponding to each node as a tuple with the id as key\n",
    "all_nodes=[]          # For storing list of node ids of all nodes\n",
    "all_elevations=[]     # For storing elevations of all nodes\n",
    "## MAYBE SAVE ALL NODE IDS IN DATAFRAME WITH ELEVATION AND BASE DEMAND AND THEN FILTER DATA FRAME LATER FOR DEMAND NODES ONLY\n",
    "\n",
    "# Creates a network model object using EPANET .inp file\n",
    "network=wntr.network.WaterNetworkModel(abs_path)\n",
    "\n",
    "# Iterates over the junction list in the Network object\n",
    "for node in network.junctions():\n",
    "    all_nodes.append(node[1].name)\n",
    "    all_elevations.append(node[1].elevation)\n",
    "    coords[node[1].name]=node[1].coordinates\n",
    "    # For all nodes that have non-zero demands\n",
    "    if node[1].base_demand != 0:\n",
    "        # Record node ID (name), desired demand (base_demand) in CMS, elevations, x and y coordinates\n",
    "        demand_nodes.append(node[1].name)\n",
    "        desired_demands.append(node[1].base_demand)\n",
    "        elevations.append(node[1].elevation)\n",
    "        \n",
    "\n",
    "conduit_ids= []       # To store IDs of the original pipes in the EPANET file\n",
    "conduit_from= []      # To store the origin node for each pipe\n",
    "conduit_to= []        # To store the destination node for each pipe\n",
    "conduit_lengths= []   # To store pipe lengths\n",
    "conduit_diameters= [] # To store pipe diameters\n",
    "\n",
    "# Loop over each link in the EPANET model\n",
    "for link in network.links():\n",
    "\n",
    "    # Extract and store each of the aforementioned properties\n",
    "    conduit_ids.append(link[1].name)\n",
    "    conduit_from.append(link[1].start_node_name)\n",
    "    conduit_to.append(link[1].end_node_name)\n",
    "    conduit_lengths.append(link[1].length)\n",
    "    conduit_diameters.append(link[1].diameter)\n",
    "\n",
    "reservoir_ids=[]      # To store the source reservoirs' IDs\n",
    "reservoir_heads={}    # To store the total head of each reservoir indexed by ID\n",
    "reservoir_coords={}   # To store the coordinates as tuple (x,y) indexed by ID\n",
    "\n",
    "# Loops over each reservoir\n",
    "for reservoir in network.reservoirs():\n",
    "    reservoir_ids.append(reservoir[1].name)\n",
    "    reservoir_heads[reservoir_ids[-1]]=reservoir[1].base_head\n",
    "    reservoir_coords[reservoir_ids[-1]]=reservoir[1].coordinates\n",
    "reservoir_elevations={reservoir:reservoir_heads[reservoir]-30 for reservoir in reservoir_heads}\n",
    "\n",
    "# Get the supply duration in minutes (/60) as an integer\n",
    "supply_duration=int(network.options.time.duration/60)\n",
    "supply_hh=str(supply_duration//60)     # The hour value of the supply duration (quotient of total supply in minutes/ 60)\n",
    "supply_mm=str(supply_duration%60)      # The minute value of the supply duration (remainder)\n",
    "\n",
    "# Corrects the formatting of the HH:MM by adding a 0 if it is a single digit: if minutes =4 -> 04\n",
    "if len(supply_mm)<2:\n",
    "    supply_mm='0'+supply_mm\n",
    "if len(supply_hh)<2:\n",
    "    supply_hh='0'+supply_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Discretization of Longer Pipes\n",
    "To improve the stability and accuracy of SWMM in complex networks, longer pipes should be discretized into smaller ones  \n",
    "Refer to [3] for a more in-depth discussion of this procedure  \n",
    "The following cell breaks down pipes that are longer than a specified maximum $\\Delta x_{max}$  \n",
    "Pipes longer than the maximum length are divided into equal parts, where the number of parts is:   \n",
    "$$ N_{parts}= \\left\\lceil \\frac{L_{pipe}}{\\Delta x_{max}} \\right\\rceil$$\n",
    "The length of each part is then set at:\n",
    "$$L_{part}=\\frac{L_{pipe}}{N_{parts}}$$\n",
    "After creating all pipe segments, intermediate nodes are created to join the pipe segments.  \n",
    "The elevation of these nodes (as well as the x and y coordinates) are interpolated linearly using the start elevation and the end elevation,  \n",
    " where the difference in elevation between each two consecutive nodes set as:\n",
    "$$\\Delta E = \\frac{ E_{end}-E_{start}}{N_{parts}}$$\n",
    "Similarly, the difference in x and y coordinates are found as:  \n",
    "$$\\Delta x = \\frac{ x_{end}-x_{start}}{N_{parts}}\\\\\n",
    "\\Delta y = \\frac{ y_{end}-y_{start}}{N_{parts}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of conduit allowed\n",
    "maximum_xdelta=10\n",
    "\n",
    "# Dataframe aggregating all node information gathered from the EPANET file\n",
    "junctions=pd.DataFrame(zip(all_nodes,all_elevations,coords.values()),columns=[\"ID\",\"Elevation\",\"Coordinates\"])\n",
    "# Set the junction ID as the index of the Dataframe\n",
    "junctions.set_index(\"ID\",inplace=True)\n",
    "\n",
    "# Dataframe aggregating all conduit information gathered from the EPANET file\n",
    "conduits=pd.DataFrame(zip(conduit_ids,conduit_from,conduit_to,conduit_lengths,conduit_diameters),columns=[\"ID\",\"from node\",\"to node\",\"Length\",\"diameter\"])\n",
    "# Set the conduit ID as the index\n",
    "conduits.set_index(\"ID\",inplace=True)\n",
    "\n",
    "# Loop over each conduit in the original file\n",
    "for conduit in conduits.index:\n",
    "\n",
    "    length=conduits[\"Length\"][conduit]  #Stores the length of the current conduit for shorthand\n",
    "\n",
    "    # If the conduit is bigger than the maximum allowable length (delta x), we will break it down into smaller pipes\n",
    "    if length>maximum_xdelta:\n",
    "        # Number of smaller pipes is calculated from \n",
    "        n_parts=math.ceil(length/maximum_xdelta)\n",
    "        # Calculate the length of each part \n",
    "        part_length=length/n_parts\n",
    "        # Start node ID (for shorthand)\n",
    "        start_node=conduits[\"from node\"][conduit]\n",
    "        # End node ID (for shorthand)\n",
    "        end_node=conduits[\"to node\"][conduit]\n",
    "        # If the start node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # MAke the start elevation the same as the end but add 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            start_elevation=junctions.at[end_node,\"Elevation\"]+1\n",
    "            reservoir_elevations[start_node]=start_elevation+1\n",
    "        # Otherwise make the start elevation equal to the elevation of the start node\n",
    "        else: start_elevation=junctions.at[start_node,\"Elevation\"]\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # MAke the end elevation the same as the start but subtract 1 (since reservoirs don't have ground elevation in EPANET)\n",
    "            end_elevation=start_elevation-1\n",
    "        # Make the end elevation equal to the elevation of the end node\n",
    "        else: end_elevation=junctions.at[end_node,\"Elevation\"]\n",
    "        # Calculate the uniform drop (or rise) in elevation for all the intermediate nodes about to be created when this pipe is broken into several smaller ones\n",
    "        unit_elev_diff=(end_elevation-start_elevation)/n_parts\n",
    "\n",
    "        # if the starting node is a reservoir\n",
    "        if start_node in reservoir_ids:\n",
    "            # Get coordinates from reservoir data\n",
    "            start_x=reservoir_coords[start_node][0]\n",
    "            start_y=reservoir_coords[start_node][1]\n",
    "        else:\n",
    "            # Get the coordinates from the junction data\n",
    "            start_x=junctions.at[start_node,\"Coordinates\"][0]\n",
    "            start_y=junctions.at[start_node,\"Coordinates\"][1]\n",
    "        \n",
    "        # If the end node is a reservoir\n",
    "        if end_node in reservoir_ids:\n",
    "            # Get the coordinates from the reservoir data\n",
    "            end_x=reservoir_coords[end_node][0]\n",
    "            end_y=reservoir_coords[end_node][1]\n",
    "        else:\n",
    "            # Get them from the junctions data\n",
    "            end_x=junctions.at[end_node,\"Coordinates\"][0]\n",
    "            end_y=junctions.at[end_node,\"Coordinates\"][1]\n",
    "            \n",
    "        # Calculate the unit difference in x and y coordinates for this pipe and its segments\n",
    "        unit_x_diff=(end_x-start_x)/n_parts\n",
    "        unit_y_diff=(end_y-start_y)/n_parts\n",
    "\n",
    "\n",
    "# THIS LOOP GENERATES THE SMALLER PIPES TO REPLACE THE ORIGINAL LONG PIPE\n",
    "        # For each part to be created\n",
    "        for part in np.arange(1,n_parts+1):\n",
    "\n",
    "            # CREATING THE LINKS\n",
    "            # Create the ID for the new smaller pipe as OriginPipeID-PartNumber\n",
    "            new_id=conduit+\"-\"+str(part)\n",
    "            # Set the new pipe's diameter equal to the original one\n",
    "            conduits.at[new_id,\"diameter\"]=conduits[\"diameter\"][conduit]\n",
    "            # Set the start node as OriginStartNode-NewNodeNumber-OriginEndNode  as in the first intermediate nodes between node 13 and 14 will be named 13-1-14\n",
    "            conduits.at[new_id,\"from node\"]=start_node+\"-\"+str(part-1)+\"-\"+end_node\n",
    "            # if this is the first part, use the original start node \n",
    "            if part==1:\n",
    "                conduits.at[new_id,\"from node\"]=start_node\n",
    "            # Set the end node as OriginStartNode-NewNodeNumber+1-OriginEndNode  as in the second intermediate nodes between node 13 and 14 will be named 13-2-14\n",
    "            conduits.at[new_id,\"to node\"]=start_node+\"-\"+str(part)+\"-\"+end_node\n",
    "            # If this is the last part, use the original end node as the end node\n",
    "            if part==n_parts:\n",
    "                conduits.at[new_id,\"to node\"]=end_node\n",
    "            # Set the new pipe's length to the length of each part\n",
    "            conduits.at[new_id,\"Length\"]=part_length\n",
    "\n",
    "            # if this is NOT the last part (as the last pipe segment joins a pre-existing node and does not need a node to be created)\n",
    "            if part<n_parts:\n",
    "                # Create a new node at the end of this pipe segment whose elevation is translated from the start elevation using the unit slope and the part number\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Elevation\"]=start_elevation+part*unit_elev_diff\n",
    "                # Calculate the coordinates for the new node using the unit difference in x and y coordinates\n",
    "                junctions.at[conduits.at[new_id,\"to node\"],\"Coordinates\"]=(start_x+part*unit_x_diff,start_y+part*unit_y_diff)\n",
    "\n",
    "        # After writing the new smaller pipes, delete the original pipe (since it is now redundant)\n",
    "        conduits.drop(conduit,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Junctions\n",
    "The junction section lines are written. Data required for the junctions include:\n",
    "Name: Already stored in junctions  \n",
    "Elevation: Already stored in junctions  \n",
    "MaxDepth: 0  \n",
    "Initial Depth (InitDepth): 0 No initialization required  \n",
    "Surcharge Depth (SurDepth): 100 or any value high enough to prevent the node from overflowing (to simulate a pressurized pipe)  \n",
    "Area Ponded (Aponded): 0  No ponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxDepth=[0]*len(junctions)\n",
    "InitDepth=MaxDepth\n",
    "SurDepth=[100] * len(junctions)  # High value to prevent surcharging\n",
    "Aponded=InitDepth\n",
    "\n",
    "# Creates dataframe with each row representing one line from the junctions section\n",
    "junctions_section=pd.DataFrame(list(zip(junctions.index,junctions[\"Elevation\"],MaxDepth,InitDepth,SurDepth,Aponded)))\n",
    "# Converts the dataframe into a list of lines in the junctions section\n",
    "junctions_section=junctions_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "# adds a new line character to the end of each line in the section\n",
    "junctions_section=[line+'\\n' for line in junctions_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outfalls\n",
    "VRW in SWMM does not require outfalls, however, SWMM does require at least 1 outfall to perform a simulation  \n",
    "Thus one artificial outfall is added, the flow to which will be negligibly set by its corresponding outlet\n",
    "Data required for outfalls:  \n",
    "Name: formatted as OutfallX where X is the ID of the original demand node  \n",
    "Elevation: equal to the original demand node's elevation  \n",
    "Type: FREE no boundary conditions forced  \n",
    "Stage Data: Blank None provided  \n",
    "Gated: NO  \n",
    "Route To: Blank None specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(demand_nodes)\n",
    "outfall_ids=[\"Outfall\"+ str(node) for node in demand_nodes]\n",
    "outfall_elevations=elevations\n",
    "outfall_types=['FREE' for i in demand_nodes]\n",
    "stage_data=[\"    \" for i in demand_nodes]\n",
    "outfall_gated=[\"NO\" for i in demand_nodes]\n",
    "# Creates dataframe with each row representing one line from the outfalls section\n",
    "outfall_section=pd.DataFrame(zip(outfall_ids,outfall_elevations,outfall_types,stage_data,outfall_gated))\n",
    "# Converts the dataframe into a list of lines in the outfalls section\n",
    "outfall_section=outfall_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "# adds a new line character to the end of each line in the section\n",
    "outfall_section=[line+'\\n' for line in outfall_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Storage Nodes\n",
    "Two types of storage nodes are needed in a VRW SWMM simulation  \n",
    "First, storage nodes that represent the constant-head sources (reservoirs) present in the original file. These are assigned large constant area values to fix their head at a constant value.  \n",
    "Second, each demand node is connected to a storage node with a defined volume curve that specifies the area as:  \n",
    "$$\n",
    "A(d)=\n",
    "\\begin{cases}\n",
    "V_{des}/h_{tank} & \\quad \\text{when}\\, 0 \\leqslant d \\leqslant h_{tank}\\\\\n",
    "\\approx 0 & \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "Where $A(d)$ is the Area of the Storage as a function of the water depth $d$, $h_{tank}$ is the specified height of the consumer tank, and $V_{des}$ is the consumer's desired volume in a supply cycle  \n",
    "In this case we use the height of the consumer tank as 1 m, but feel free to change that if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Storage\n",
    "tank_height=1\n",
    "\n",
    "storage_ids=[\"StorageforNode\"+id for id in demand_nodes]\n",
    "storage_areas=[demand*60* supply_duration/tank_height for demand in desired_demands]\n",
    "storage_elevations=elevations\n",
    "storage_curves=[round(volume*10000) for volume in storage_areas]\n",
    "storage_MaxDepth=[max(100,max(reservoir_heads.values()))]*len(storage_ids)\n",
    "storage_InitDepth=[0]*len(storage_ids)\n",
    "storage_shape=[\"TABULAR\"]*len(storage_ids)\n",
    "blanks=['    ']*len(storage_ids)    # for the other curve parameters which are not required in Tabular definition\n",
    "storage_SurDepth=[0]*len(storage_ids)\n",
    "storage_fevap=[0]*len(storage_ids)\n",
    "\n",
    "storage_units=pd.DataFrame(zip(storage_ids,storage_elevations,storage_MaxDepth,storage_InitDepth,storage_shape,storage_curves,blanks,blanks,storage_SurDepth,storage_fevap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Flow Storage\n",
    "ovf_ids=[\"OverflowforNode\"+id for id in demand_nodes]\n",
    "ovf_area=1\n",
    "ovf_elevations=elevations\n",
    "ovf_curves=[\"Overflow\" for volume in storage_areas]\n",
    "ovf_MaxDepth=[1]*len(storage_ids)\n",
    "ovf_InitDepth=[0]*len(storage_ids)\n",
    "ovf_shape=[\"TABULAR\"]*len(storage_ids)\n",
    "blanks=['    ']*len(storage_ids)    # for the other curve parameters which are not required in Tabular definition\n",
    "ovf_SurDepth=[0]*len(storage_ids)\n",
    "ovf_fevap=[0]*len(storage_ids)\n",
    "\n",
    "ovf_units=pd.DataFrame(zip(ovf_ids,ovf_elevations,ovf_MaxDepth,ovf_InitDepth,ovf_shape,ovf_curves,blanks,blanks,ovf_SurDepth,ovf_fevap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_ids_new=[\"Reservoir-\"+str(id) for id in reservoir_ids]\n",
    "for i in range(0,len(reservoir_ids)):\n",
    "    conduits.replace(to_replace=reservoir_ids[i],value=reservoir_ids_new[i])\n",
    "reservoir_elevations=reservoir_elevations.values()\n",
    "MaxDepth=[max(100,max(reservoir_heads.values())+10)]*len(reservoir_ids)\n",
    "InitDepth=[head-elevation for head, elevation in zip(reservoir_heads.values(),reservoir_elevations)]\n",
    "reservoir_shape=[\"FUNCTIONAL\"]*len(reservoir_ids)\n",
    "reservoir_coeff=[0]*len(reservoir_ids)\n",
    "reservoir_expon=[0]*len(reservoir_ids)\n",
    "reservoir_const=[1000000]*len(reservoir_ids)\n",
    "reservoir_SurDepth=reservoir_expon\n",
    "reservoir_psi=reservoir_expon\n",
    "\n",
    "storage_section=pd.DataFrame(zip(reservoir_ids,reservoir_elevations,MaxDepth,InitDepth,reservoir_shape,reservoir_coeff,reservoir_expon,reservoir_const,reservoir_SurDepth,reservoir_psi))\n",
    "storage_section= pd.concat([storage_section,storage_units,ovf_units])\n",
    "storage_section=storage_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "storage_section=[line+'\\n' for line in storage_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Conduits\n",
    "Name From To L Roughness(manning) InOff  OutOff InitFlow  MaxFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughness=[0.011]*len(conduits)\n",
    "conduit_zeros=[0]*len(conduits)\n",
    "\n",
    "conduits_section=pd.DataFrame(zip(conduits.index,conduits[\"from node\"],conduits[\"to node\"],conduits[\"Length\"],roughness,conduit_zeros,conduit_zeros,conduit_zeros,conduit_zeros))\n",
    "conduits_section=conduits_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "conduits_section=[line+'\\n' for line in conduits_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Outlets\n",
    "To have a rating curve with no upper limit, we use the FUNCTIONAL mode of definition in relation to the depth (Pressure) at the original demand node  \n",
    "Functional definition of rating curves is specified as:\n",
    "$$ Q_{outlet}=c d^{\\alpha}$$\n",
    "where d is teh depth, c is a coefficient and alpha is an exponent  \n",
    "We aim to approximate the head-flow relationship (above) using this definition, by using the same exponent 0.5 and calculating the coefficient such that:  \n",
    "$$ Q=Q_{des} \\text{ when } d=H^{des}-H^{min}$$\n",
    "Thus the function's coefficient can be found as:  \n",
    "$$ c=\\frac{Q_{des}}{\\sqrt{H^{des}-H^{min}}}$$\n",
    "Since the depth at the node is equal to $H_j$, this approximation is exact when $H_{min}=0$ \n",
    "$$ Q_{outlet}=Q_{des}\\sqrt{\\frac{H_j}{{H^{des}-H^{min}}}}$$\n",
    "Lastly, an outlet is needed to restrict the flow to the artificial outfall to a negligible amount by setting the coefficient to a sufficiently small value and the exponent to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Outlet\n",
    "outlet_ids = [\"Outlet\"+id for id in demand_nodes]\n",
    "outlet_from = demand_nodes[:]\n",
    "outlet_to = storage_ids [:]\n",
    "outlet_offset=[0]*len(outlet_ids)\n",
    "outlet_type=[\"FUNCTIONAL/DEPTH\"]*len(outlet_ids)\n",
    "outlet_coeff=[demand*1000/np.sqrt(pressure_diff) for demand in desired_demands]  # To generate unique Table IDs for each demand rate (not demand node) i.e., juncitons with the same demand are assigned the same outlet curve\n",
    "outlet_expon=[\"0.5\"]*len(outlet_ids)\n",
    "outlet_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "# Overflow Outlet\n",
    "outovf_ids = [\"OverflowOutlet\"+id for id in demand_nodes]\n",
    "outovf_from = storage_ids[:]\n",
    "outovf_to = ovf_ids [:]\n",
    "outovf_offset=[tank_height]*len(outlet_ids)\n",
    "outovf_type=[\"TABULAR/DEPTH\"]*len(outlet_ids)\n",
    "outovf_coeff=[\"Drain\"]*len(outovf_ids) \n",
    "outovf_expon=[\"     \"]*len(outlet_ids)\n",
    "outovf_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "# Demand Outlet\n",
    "outdemand_ids = [\"DemandOutlet\"+id for id in demand_nodes]\n",
    "outdemand_from = storage_ids [:]\n",
    "outdemand_to = outfall_ids [:]\n",
    "outdemand_offset=[0]*len(outlet_ids)\n",
    "outdemand_type=[\"TABULAR/DEPTH\"]*len(outlet_ids)\n",
    "outdemand_coeff=[\"Demand\"+ id for id in demand_nodes]\n",
    "outdemand_expon=[\"     \"]*len(outlet_ids)\n",
    "outdemand_gated=[\"YES\"]*len(outlet_ids)\n",
    "\n",
    "outlets=pd.DataFrame(list(zip(outlet_ids,outlet_from,outlet_to,outlet_offset,outlet_type,outlet_coeff,outlet_expon,outlet_gated)))\n",
    "outletovfs=pd.DataFrame(list(zip(outovf_ids,outovf_from,outovf_to,outovf_offset,outovf_type,outovf_coeff,outovf_expon,outovf_gated)))\n",
    "outletdemand=pd.DataFrame(list(zip(outdemand_ids,outdemand_from,outdemand_to,outdemand_offset,outdemand_type,outdemand_coeff,outdemand_expon,outdemand_gated)))\n",
    "outlet_section=pd.concat([outlets,outletovfs,outletdemand])\n",
    "outlet_section=outlet_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "outlet_section=[line+'\\n' for line in outlet_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Pumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pump_ids = [\"Pump\" + id for id in demand_nodes]\n",
    "pump_from = ovf_ids\n",
    "pump_to = demand_nodes\n",
    "pump_curves = [\"OVERFLOW_PUMP\"]*len(pump_ids)\n",
    "pump_status = [\"ON\"]*len(pump_ids)\n",
    "pump_startup = [0]*len(pump_ids)\n",
    "pump_shutoff = [0]*len(pump_ids)\n",
    "\n",
    "pumps=pd.DataFrame(list(zip(pump_ids,pump_from,pump_to,pump_curves,pump_status,pump_startup,pump_shutoff)))\n",
    "pumps_section=pumps.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "pumps_section=[line+'\\n'for line in pumps_section]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing X-Sections\n",
    "Link    Shape     Geom1 (DIAMETER)    Geom2 (HW Coefficient)     Geom3    Geom 4       Barrels     Culvert (EMPTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=[\"FORCE_MAIN\"]*len(conduits.index)\n",
    "hwcoeffs=[130]*len(shape)\n",
    "geom3=[0]*len(shape)\n",
    "geom4=geom3\n",
    "nbarrels=[1]*len(shape)\n",
    "\n",
    "xsections_section=pd.DataFrame(zip(conduits.index,shape,conduits[\"diameter\"],hwcoeffs,geom3,geom4,nbarrels))\n",
    "xsections_section=xsections_section.to_string(header=False,index=False, col_space=10).splitlines()\n",
    "xsections_section=[line+'\\n' for line in xsections_section]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Curves\n",
    "Storage Curves  \n",
    "Outlet Curves  \n",
    "Pump Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ids=list(set(storage_curves))   # removes duplicates from list\n",
    "curves_name=[]\n",
    "curves_type=[]\n",
    "curves_x=[]\n",
    "curves_y=[]\n",
    "for table in table_ids:\n",
    "    volume=int(table)/10000                # in LPS\n",
    "    for depth in [0,1,1.0001,100]:\n",
    "        curves_name.append(table)\n",
    "        curves_x.append(depth)\n",
    "        if depth<=1:\n",
    "            curves_y.append(volume)\n",
    "        else: curves_y.append(1)\n",
    "        if depth==0:\n",
    "            curves_type.append(\"Storage\")\n",
    "        else: \n",
    "            curves_type.append(\" \")\n",
    "    curves_name.append(\";\")\n",
    "    curves_type.append(\" \")\n",
    "    curves_x.append(\" \")\n",
    "    curves_y.append(\" \")\n",
    "curves_name+=[\"Overflow\",\"Overflow\",';']\n",
    "curves_type+=[\"Storage\",\"   \",';']\n",
    "curves_x+=[0,1,';']\n",
    "curves_y+=[1,1,';']\n",
    "\n",
    "\n",
    "curves_name+=[\"OVERFLOW_PUMP\",\"OVERFLOW_PUMP\",';']\n",
    "curves_type+=[\"Pump4\",\"   \",' ']\n",
    "curves_x+=[0,1,' ']\n",
    "curves_y+=[0,1000,' ']\n",
    "\n",
    "curves_name+=[\"Drain\",\"Drain\",';']\n",
    "curves_type+=[\"Rating\",\"   \",' ']\n",
    "curves_x+=[0,1,' ']\n",
    "curves_y+=[0,1000,' ']\n",
    "\n",
    "constant_demands=[demand*float(supply_hh)/24 for demand in desired_demands]\n",
    "for i,j in zip(demand_nodes,constant_demands):\n",
    "    curves_name+=[\"Demand\"+str(i),\"Demand\"+str(i),';']\n",
    "    curves_type+=[\"Rating\",\" \",\"  \"]\n",
    "    curves_x+=[0,0.01,\" \"]\n",
    "    curves_y+=[0,j*1000,\" \"]\n",
    "\n",
    "curves=pd.DataFrame(list(zip(curves_name,curves_type,curves_x,curves_y)))\n",
    "curves_section=curves.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "curves_section=[line+'\\n' for line in curves_section]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_demand= { node: coords[node] for node in demand_nodes}\n",
    "coords_ids=list(junctions.index)+reservoir_ids+storage_ids+ovf_ids+outfall_ids\n",
    "\n",
    "coords_x1=[coord[0] for coord in junctions[\"Coordinates\"]]\n",
    "coords_x2=[coord[0] for coord in reservoir_coords.values()]\n",
    "coords_x3=[coord[0] +20 for coord in coords_demand.values()]\n",
    "coords_x4=[coord[0] +10 for coord in coords_demand.values()]\n",
    "coords_x5=[coord[0] +30 for coord in coords_demand.values()]\n",
    "coords_x=coords_x1+coords_x2+coords_x3+coords_x4+coords_x5\n",
    "\n",
    "coords_y1=[coord[1] for coord in junctions[\"Coordinates\"]]\n",
    "coords_y2=[coord[1] for coord in reservoir_coords.values()]\n",
    "coords_y3=[coord[1] for coord in coords_demand.values()]\n",
    "coords_y4=[coord[1]+10 for coord in coords_demand.values()]\n",
    "coords_y5=[coord[1] for coord in coords_demand.values()]\n",
    "coords_y=coords_y1+coords_y2+coords_y3+coords_y4+coords_y5\n",
    "\n",
    "coordinate_section=pd.DataFrame(zip(coords_ids,coords_x,coords_y))\n",
    "coordinate_section=coordinate_section.to_string(header=False,index=False,col_space=10).splitlines()\n",
    "coordinate_section=[line+'\\n' for line in coordinate_section]\n",
    "\n",
    "#Setting View Dimensions\n",
    "x_left=min(coords_x)-max(coords_x)/4\n",
    "x_right=max(coords_x)+max(coords_x)/4\n",
    "y_down=min(coords_y)-max(coords_y)/4\n",
    "y_up=max(coords_y)+max(coords_y)/4\n",
    "dimensions_line=str(x_left)+\" \"+str(y_down)+\" \"+str(x_right)+\" \"+str(y_up)+\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the SWMM .inp File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens .inp file to read\n",
    "file=open(r\"C:\\IWS_Modelling\\Github\\IWS-Modelling-Methods-Repo\\Network-Files\\EMPTY_SWMM_TEMPLATE.inp\",'r')\n",
    "lines=[]              # list to store all lines in the .inp file\n",
    "linecount=0           # Counter for the number of lines\n",
    "\n",
    "# Loops over each line in the input file \n",
    "for line in file:\n",
    "    if re.search(\"^END_TIME\",line):\n",
    "        end_time=linecount\n",
    "    if re.search(\"^DIMENSIONS\",line):\n",
    "        dimensions=linecount\n",
    "    # Record the position of the phrase [JUNCTIONS] and add 3 to skip the header lines\n",
    "    if re.search('\\[JUNCTIONS\\]',line):\n",
    "        junctions_marker=linecount+3\n",
    "    # Record the position of the phrase [OUTFALLS] and add 3 to skip the header lines\n",
    "    if re.search('\\[OUTFALLS\\]',line):\n",
    "        outfalls_marker=linecount+3\n",
    "    # Record the position of the phrase [STORAGE] and add 3 to skip the header lines\n",
    "    if re.search('\\[STORAGE\\]',line):\n",
    "        storage_marker=linecount+3\n",
    "    # Record the position of the phrase [CONDUITS] and add 3 to skip the header lines\n",
    "    if re.search('\\[CONDUITS\\]',line):\n",
    "        conduits_marker=linecount+3\n",
    "     # Record the position of the phrase [OUTLETS] and add 3 to skip the header lines\n",
    "    if re.search('\\[PUMPS\\]',line):\n",
    "        pumps_marker=linecount+3\n",
    "    if re.search('\\[OUTLETS\\]',line):\n",
    "        outlets_marker=linecount+3\n",
    "     # Record the position of the phrase [XSECTIONS] and add 3 to skip the header lines\n",
    "    if re.search('\\[XSECTIONS\\]',line):\n",
    "        xsections_marker=linecount+3\n",
    "    # Record the position of the phrase [CURVES] and add 3 to skip the header lines\n",
    "    if re.search('\\[CURVES\\]',line):\n",
    "        curves_marker=linecount+3\n",
    "    # Record the position of the phrase [COORDINATES] and add 3 to skip the header lines\n",
    "    if re.search('\\[COORDINATES\\]',line):\n",
    "        coords_marker=linecount+3\n",
    "    # Store all lines in a list\n",
    "    lines.append(line)\n",
    "    linecount+=1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(directory+name_only+'_3O2S1P.inp','w')\n",
    "lines[end_time]=\"END_TIME             \"+str(23)+\":\"+str(59)+\":00\\n\"\n",
    "lines[dimensions]=\"DIMENSIONS \"+dimensions_line\n",
    "lines[coords_marker:coords_marker]=coordinate_section\n",
    "lines[curves_marker:curves_marker]=curves_section\n",
    "lines[xsections_marker:xsections_marker]=xsections_section\n",
    "lines[outlets_marker:outlets_marker]=outlet_section\n",
    "lines[pumps_marker:pumps_marker]=pumps_section\n",
    "lines[conduits_marker:conduits_marker]=conduits_section\n",
    "lines[storage_marker:storage_marker]=storage_section\n",
    "lines[outfalls_marker:outfalls_marker]=outfall_section\n",
    "lines[junctions_marker:junctions_marker]=junctions_section\n",
    "\n",
    "\n",
    "\n",
    "# All lines added by this script are missing a new line character at the end, the conditional statements below add the new line character for these lines only and writes all lines to the file\n",
    "for line in lines:\n",
    "    file.write(line)    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
